---
title: "Algorithms"
subtitle: "Algorithms and computational complexity (middle)"
author: "Helena Iréne Köhler"
date: "2024-01-08"
---

## Computational Complexity Theory and Big O Notation

![Search cat](/posts/assets/entries/algorithms/search_cat.png)
![Search cat](/posts/assets/entries/algorithms/find_cat.png)

Computational Complexity Theory is a part of theoretical computer science and revolves around how hard it is for a computer to solve problems. The complexity is characterised by the computational cost of solving a problem, the difficulty of solving a problem in terms of the computational resources required, such as computational time and memory requirements. The computational complexity of a problem is the computational complexity of the most efficient algorithm used to solve it.

For example, consider an algorithm attempting to find a Schrödinger’s cat in a number of boxes. In order to find the cat, the algorithm has to open all of the boxes. This process would be classified as O(n), with n being the number of boxes and therefore the function domain. 
The Big O Notation is a mathematical concept used to describe the limiting behavior of a function when tending towards a particular value or simply growing larger. In computer science, and thereby also here, this notation is used to describe the computational cost (time or space complexity) of an algorithm, respective to the input becoming increasingly larger. Because what happens if, instead of having to search through 5 boxes to find the cat, we have to look through thousands or a million?  
Our “algorithm” to the left isn’t very good, but it also isn’t awful, as one can depict from the Big-O Complexity chart, the number of operations increases linearly with the number of boxes: 

![Big O Notation](/posts/assets/entries/algorithms/bigo.png)

## Grover’s Algorithm

Now, let’s consider our earlier question again: how do I find a cat in a million boxes the most computationally efficient, in regards to the Big O notation and the computational complexity chart? A simple algorithm to find the cat would be, as we did before, to search all possible boxes and keep a running record of all the ones that were already opened. On a classical computer it would take O(n) operations to check each box one by one, which, as depicted on the right, would take 1.000.000 steps on average. With increasing input, the complexity of the algorithm becomes more important. 

Grover’s algorithm, also known as quantum search algorithm offers a quadratic advantage to this method by speeding up the classical approach, requiring only O( {\sqrt{n}} ) operations. The number of operations is thus reduced to around 1.000 steps (n = 1.000.000, O({\sqrt{n}}) = {\sqrt{1.000.000}} = 1.000), as visualized on the left. This significant reduction in computational complexity makes Grover's algorithm much more efficient for large-scale searches.
The algorithm was developed by Lov Grover in 1996 and was the second major quantum algorithm proposed and is an important example of how quantum computing can offer advantages over classical computing for certain types of problems. 

![Grovers' Algorithm](/posts/assets/entries/algorithms/grover.png)